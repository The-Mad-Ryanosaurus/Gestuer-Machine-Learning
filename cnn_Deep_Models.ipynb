{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries and Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, applications\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomFlip, Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "\n",
    "# Suppress warnings from the logging module\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tensorflow Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPU Checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if any GPU devices are detected\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tensorflow Warning Suppression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging except for fatal errors.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 338424\n",
    "\n",
    "# Global variables\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 18 # Number of folders in dataset\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset: Loading, Splitting, Shuffling, Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the Training set\n",
      "Using 25184 samples in the Validation set\n",
      "Using 12600 samples in the Test set\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size = len(full_ds)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - (train_size + val_size)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds = full_ds.take(train_size).shuffle(train_size, seed=SEED)\n",
    "val_ds = full_ds.skip(train_size).take(val_size).shuffle(val_size, seed=SEED)\n",
    "test_ds = full_ds.skip(train_size + val_size).shuffle(test_size, seed=SEED)\n",
    "\n",
    "# Cache the dataset in memory (or use a directory to store it on disk if necessary)\n",
    "train_ds = full_ds.take(train_size).shuffle(train_size, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = full_ds.skip(train_size).take(val_size).shuffle(val_size, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = full_ds.skip(train_size + val_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples(train_ds)} samples in the Training set')\n",
    "print(f'Using {count_samples(val_ds)} samples in the Validation set')\n",
    "print(f'Using {count_samples(test_ds)} samples in the Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset: Deep - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = 'savedDatasetDeepRGB'\n",
    "\n",
    "tf.data.experimental.save(train_ds, path_to_save + '/train')\n",
    "tf.data.experimental.save(val_ds, path_to_save + '/val')\n",
    "tf.data.experimental.save(test_ds, path_to_save + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'dislike',\n",
       " 'fist',\n",
       " 'four',\n",
       " 'like',\n",
       " 'mute',\n",
       " 'ok',\n",
       " 'one',\n",
       " 'palm',\n",
       " 'peace',\n",
       " 'peace_inverted',\n",
       " 'rock',\n",
       " 'stop',\n",
       " 'stop_inverted',\n",
       " 'three',\n",
       " 'three2',\n",
       " 'two_up',\n",
       " 'two_up_inverted']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = full_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regularization Factors**\n",
    "\n",
    "This code snippet defines the values for L1 and L2 regularization, which are both set to 0.01. It then creates an \"Elastic Net Regularizer\" that combines these L1 and L2 values to help prevent the model from overfitting by penalizing overly complex or large weight values in the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L1 and L2 regularization factors\n",
    "l1_factor = 0.01  # Example value\n",
    "l2_factor = 0.01  # Example value\n",
    "\n",
    "# Elastic Net Regularizer\n",
    "elastic_net_regularizer = regularizers.l1_l2(l1=l1_factor, l2=l2_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Callbacks: Learning Rate Scheduler and Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def lr_time_based_decay(epoch, lr):\n",
    "    # This function adjusts the learning rate over each epoch based on the initial learning rate,\n",
    "    # applying a decay factor that increases with the epoch number. It effectively reduces the \n",
    "    # learning rate over time, which can help in calibrating the model adjustments as it \n",
    "    # approaches a minimum in the loss surface.\n",
    "    return lr * 1 / (1 + 0.01 * epoch)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # EarlyStopping prevents overfitting by stopping training when the validation loss \n",
    "    # has not improved for 3 consecutive epochs ('patience=3'). It also restores the \n",
    "    # weights of the model to those of the epoch with the best validation loss, ensuring \n",
    "    # the model retains the best learned features even if it starts to overfit afterward.\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    # LearningRateScheduler adjusts the learning rate according to the lr_time_based_decay function above.\n",
    "    # It logs the new learning rate at the start of each epoch ('verbose=1'), helping to control\n",
    "    # the step size of model updates, which can be crucial for reaching convergence efficiently.\n",
    "    LearningRateScheduler(lr_time_based_decay, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Convolutional Neural Networks (CNN): Deep Models**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Deep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 2, 2, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,586,034\n",
      "Trainable params: 1,584,018\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN Model from Scratch\n",
    "def build_scratch_cnn_deep():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    \n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with an initial learning rate\n",
    "    model.compile(\n",
    "        # Adam optimizer is used with a specified initial learning rate of 0.001. The learning rate\n",
    "        # controls how much the weights of the model are adjusted relative to the gradient of the loss \n",
    "        # function. A higher learning rate might converge quickly, but too high can cause the training \n",
    "        # to diverge. A lower learning rate ensures more reliable convergence but at the risk of slowing\n",
    "        # down the training process. The chosen rate of 0.001 is a starting point that balances these factors.\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_deep = build_scratch_cnn_deep()\n",
    "scratch_model_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 36s 12ms/step - loss: 1.7865 - accuracy: 0.4529 - val_loss: 1.0698 - val_accuracy: 0.6572 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 32s 11ms/step - loss: 0.8583 - accuracy: 0.7253 - val_loss: 0.6559 - val_accuracy: 0.7915 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.6545 - accuracy: 0.7885 - val_loss: 0.5658 - val_accuracy: 0.8172 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.5396 - accuracy: 0.8242 - val_loss: 0.5175 - val_accuracy: 0.8349 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.4665 - accuracy: 0.8472 - val_loss: 0.4684 - val_accuracy: 0.8528 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.4085 - accuracy: 0.8654 - val_loss: 0.4407 - val_accuracy: 0.8630 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.3545 - accuracy: 0.8820 - val_loss: 0.4073 - val_accuracy: 0.8743 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.3157 - accuracy: 0.8943 - val_loss: 0.4476 - val_accuracy: 0.8593 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 0.2782 - accuracy: 0.9060 - val_loss: 0.3960 - val_accuracy: 0.8802 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 30s 11ms/step - loss: 0.2426 - accuracy: 0.9184 - val_loss: 0.4000 - val_accuracy: 0.8821 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_deep = scratch_model_deep.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CNN Model: Deep Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_deep.history.keys():\n",
    "    history_deep.history[key] = [float(i) for i in history_deep.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/cnn_model_deep.json', 'w') as f:\n",
    "    json.dump(history_deep.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "scratch_model_deep.save('h5/scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Shallow Calibrated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64, 64, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 32, 32, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 2, 2, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 1, 512)         262656    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,848,690\n",
      "Trainable params: 1,846,674\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN Model from Scratch\n",
    "def build_scratch_cnn_deep_calibrated():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    \n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with an initial learning rate\n",
    "    model.compile(\n",
    "        # Adam optimizer is used with a specified initial learning rate of 0.001. The learning rate\n",
    "        # controls how much the weights of the model are adjusted relative to the gradient of the loss \n",
    "        # function. A higher learning rate might converge quickly, but too high can cause the training \n",
    "        # to diverge. A lower learning rate ensures more reliable convergence but at the risk of slowing\n",
    "        # down the training process. The chosen rate of 0.001 is a starting point that balances these factors.\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_deep_calibrated = build_scratch_cnn_deep_calibrated()\n",
    "scratch_model_deep_calibrated.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep Calibrated - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 35s 12ms/step - loss: 2.1597 - accuracy: 0.2965 - val_loss: 1.3207 - val_accuracy: 0.5808 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 35s 13ms/step - loss: 1.1556 - accuracy: 0.6290 - val_loss: 0.7653 - val_accuracy: 0.7559 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 0.8540 - accuracy: 0.7295 - val_loss: 0.6372 - val_accuracy: 0.7953 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 0.6997 - accuracy: 0.7778 - val_loss: 0.7292 - val_accuracy: 0.7661 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 0.6130 - accuracy: 0.8059 - val_loss: 0.5227 - val_accuracy: 0.8339 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 0.5431 - accuracy: 0.8261 - val_loss: 0.4642 - val_accuracy: 0.8526 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 0.4864 - accuracy: 0.8449 - val_loss: 0.5348 - val_accuracy: 0.8336 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 0.4429 - accuracy: 0.8586 - val_loss: 0.4345 - val_accuracy: 0.8658 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 0.3985 - accuracy: 0.8717 - val_loss: 0.4267 - val_accuracy: 0.8702 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 34s 12ms/step - loss: 0.3645 - accuracy: 0.8810 - val_loss: 0.4610 - val_accuracy: 0.8607 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_deep_calibrated = scratch_model_deep_calibrated.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CNN Model: Deep Calibrated - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_deep_calibrated.history.keys():\n",
    "    history_deep_calibrated.history[key] = [float(i) for i in history_deep_calibrated.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/cnn_model_deep_calibrated.json', 'w') as f:\n",
    "    json.dump(history_deep_calibrated.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "scratch_model_deep_calibrated.save('h5/scratch_model_deep_calibrated.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Deep Calibrated Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling_3 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64, 64, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 32, 32, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 4, 4, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 2, 2, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 1, 1, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1, 1, 512)         262656    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,848,690\n",
      "Trainable params: 1,846,674\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN Model from Scratch\n",
    "def build_scratch_cnn_deep_calibrated_da():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    \n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=elastic_net_regularizer))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with an initial learning rate\n",
    "    model.compile(\n",
    "        # Adam optimizer is used with a specified initial learning rate of 0.001. The learning rate\n",
    "        # controls how much the weights of the model are adjusted relative to the gradient of the loss \n",
    "        # function. A higher learning rate might converge quickly, but too high can cause the training \n",
    "        # to diverge. A lower learning rate ensures more reliable convergence but at the risk of slowing\n",
    "        # down the training process. The chosen rate of 0.001 is a starting point that balances these factors.\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_deep_calibrated_da = build_scratch_cnn_deep_calibrated_da()\n",
    "scratch_model_deep_calibrated_da.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep Calibrated Data Augmentation - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 329s 119ms/step - loss: 4.7639 - accuracy: 0.1884 - val_loss: 2.4763 - val_accuracy: 0.4022 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 317s 115ms/step - loss: 2.2280 - accuracy: 0.4962 - val_loss: 1.7835 - val_accuracy: 0.6508 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 338s 123ms/step - loss: 1.7780 - accuracy: 0.6415 - val_loss: 1.4739 - val_accuracy: 0.7299 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 321s 117ms/step - loss: 1.5761 - accuracy: 0.6938 - val_loss: 1.3302 - val_accuracy: 0.7738 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 324s 117ms/step - loss: 1.4458 - accuracy: 0.7282 - val_loss: 1.1434 - val_accuracy: 0.8205 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 324s 118ms/step - loss: 1.3460 - accuracy: 0.7487 - val_loss: 1.0689 - val_accuracy: 0.8355 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 323s 117ms/step - loss: 1.2406 - accuracy: 0.7687 - val_loss: 0.9761 - val_accuracy: 0.8495 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 308s 112ms/step - loss: 1.1745 - accuracy: 0.7806 - val_loss: 1.0803 - val_accuracy: 0.8138 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 318s 116ms/step - loss: 1.1046 - accuracy: 0.7932 - val_loss: 0.9891 - val_accuracy: 0.8304 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 318s 116ms/step - loss: 1.0424 - accuracy: 0.8030 - val_loss: 0.8824 - val_accuracy: 0.8573 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_deep_calibrated_da = scratch_model_deep_calibrated_da.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CNN Model: Deep Calibrated Data Augmentation - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_deep_calibrated_da.history.keys():\n",
    "    history_deep_calibrated_da.history[key] = [float(i) for i in history_deep_calibrated_da.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/cnn_model_deep_calibrated_da.json', 'w') as f:\n",
    "    json.dump(history_deep_calibrated_da.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "scratch_model_deep_calibrated_da.save('h5/scratch_model_deep_calibrated_da.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Deep Grayscale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grayscale Dataset: Loading, Splitting, Shuffling, Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the Training set Grayscale\n",
      "Using 25184 samples in the Validation set Grayscale\n",
      "Using 12600 samples in the Test set Grayscale\n"
     ]
    }
   ],
   "source": [
    "# Load Grayscale Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds_grayscale = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale'  # Load images as grayscale\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio_grayscale = 0.7\n",
    "val_ratio_grayscale = 0.2\n",
    "test_ratio_grayscale = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size_grayscale = len(full_ds_grayscale)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size_grayscale = int(total_size_grayscale * train_ratio_grayscale)\n",
    "val_size_grayscale = int(total_size_grayscale * val_ratio_grayscale)\n",
    "test_size_grayscale = total_size_grayscale - (train_size_grayscale + val_size_grayscale)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds_grayscale = full_ds_grayscale.take(train_size_grayscale).shuffle(train_size_grayscale, seed=SEED)\n",
    "val_ds_grayscale = full_ds_grayscale.skip(train_size_grayscale).take(val_size_grayscale).shuffle(val_size_grayscale, seed=SEED)\n",
    "test_ds_grayscale = full_ds_grayscale.skip(train_size_grayscale + val_size_grayscale).shuffle(test_size_grayscale, seed=SEED)\n",
    "\n",
    "# Cache the dataset in memory (or use a directory to store it on disk if necessary)\n",
    "train_ds_grayscale = full_ds_grayscale.take(train_size_grayscale).shuffle(train_size_grayscale, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_grayscale = full_ds_grayscale.skip(train_size_grayscale).take(val_size_grayscale).shuffle(val_size_grayscale, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds_grayscale = full_ds_grayscale.skip(train_size_grayscale + val_size_grayscale).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples(train_ds)} samples in the Training set Grayscale')\n",
    "print(f'Using {count_samples(val_ds)} samples in the Validation set Grayscale')\n",
    "print(f'Using {count_samples(test_ds)} samples in the Test set Grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'dislike',\n",
       " 'fist',\n",
       " 'four',\n",
       " 'like',\n",
       " 'mute',\n",
       " 'ok',\n",
       " 'one',\n",
       " 'palm',\n",
       " 'peace',\n",
       " 'peace_inverted',\n",
       " 'rock',\n",
       " 'stop',\n",
       " 'stop_inverted',\n",
       " 'three',\n",
       " 'three2',\n",
       " 'two_up',\n",
       " 'two_up_inverted']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = full_ds_grayscale.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset: Deep Grayscale - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Saved to analyse Inference Times in Evaluation Notebook\n",
    "path_to_save = 'savedDatasetDeepGrayscale'\n",
    "\n",
    "tf.data.experimental.save(train_ds_grayscale, path_to_save + '/train')\n",
    "tf.data.experimental.save(val_ds_grayscale, path_to_save + '/val')\n",
    "tf.data.experimental.save(test_ds_grayscale, path_to_save + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64, 64, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 32, 32, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 2, 2, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 1, 512)         262656    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,848,402\n",
      "Trainable params: 1,846,386\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN Model from Scratch\n",
    "def build_scratch_cnn_deep_grayscale():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1)))\n",
    "    # model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    \n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=elastic_net_regularizer))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with an initial learning rate\n",
    "    model.compile(\n",
    "        # Adam optimizer is used with a specified initial learning rate of 0.001. The learning rate\n",
    "        # controls how much the weights of the model are adjusted relative to the gradient of the loss \n",
    "        # function. A higher learning rate might converge quickly, but too high can cause the training \n",
    "        # to diverge. A lower learning rate ensures more reliable convergence but at the risk of slowing\n",
    "        # down the training process. The chosen rate of 0.001 is a starting point that balances these factors.\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_deep_grayscale = build_scratch_cnn_deep_grayscale()\n",
    "scratch_model_deep_grayscale.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep Grayscale - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 138s 25ms/step - loss: 4.7776 - accuracy: 0.1944 - val_loss: 2.9313 - val_accuracy: 0.2982 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 34s 12ms/step - loss: 2.2628 - accuracy: 0.4904 - val_loss: 1.8157 - val_accuracy: 0.6308 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 1.8325 - accuracy: 0.6257 - val_loss: 1.6091 - val_accuracy: 0.6893 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 1.6228 - accuracy: 0.6850 - val_loss: 1.4891 - val_accuracy: 0.7210 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 1.4823 - accuracy: 0.7209 - val_loss: 1.3026 - val_accuracy: 0.7671 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 33s 12ms/step - loss: 1.3743 - accuracy: 0.7459 - val_loss: 1.3212 - val_accuracy: 0.7560 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 34s 12ms/step - loss: 1.2780 - accuracy: 0.7647 - val_loss: 1.2200 - val_accuracy: 0.7810 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 32s 12ms/step - loss: 1.1971 - accuracy: 0.7800 - val_loss: 1.1524 - val_accuracy: 0.7915 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 31s 11ms/step - loss: 1.1244 - accuracy: 0.7936 - val_loss: 1.1191 - val_accuracy: 0.7933 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 30s 11ms/step - loss: 1.0526 - accuracy: 0.8069 - val_loss: 1.0186 - val_accuracy: 0.8140 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_deep_grayscale = scratch_model_deep_grayscale.fit(\n",
    "    train_ds_grayscale,\n",
    "    validation_data=val_ds_grayscale,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CNN Model: Deep Grayscale - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_deep_grayscale.history.keys():\n",
    "    history_deep_grayscale.history[key] = [float(i) for i in history_deep_grayscale.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/cnn_model_deep_grayscale.json', 'w') as f:\n",
    "    json.dump(history_deep_grayscale.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "scratch_model_deep_grayscale.save('h5/scratch_model_deep_grayscale.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Deep High Resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High resolution images\n",
    "# Image size changed from 64x64 to 128x128\n",
    "IMG_SIZE_HR = (128, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **High Resolution Dataset: Loading, Splitting, Shuffling, Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the High Training set High Resolution\n",
      "Using 25184 samples in the High Validation set High Resolution\n",
      "Using 12600 samples in the High Test set High Resolution\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds_hr = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE_HR),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio_hr = 0.7\n",
    "val_ratio_hr = 0.2\n",
    "test_ratio_hr = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size_hr = len(full_ds_hr)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size_hr = int(total_size_hr * train_ratio_hr)\n",
    "val_size_hr = int(total_size_hr * val_ratio_hr)\n",
    "test_size_hr = total_size_hr - (train_size_hr + val_size_hr)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds_hr = full_ds_hr.take(train_size_hr).shuffle(train_size_hr, seed=SEED)\n",
    "val_ds_hr = full_ds_hr.skip(train_size_hr).take(val_size_hr).shuffle(val_size_hr, seed=SEED)\n",
    "test_ds_hr = full_ds_hr.skip(train_size_hr + val_size_hr).shuffle(test_size_hr, seed=SEED)\n",
    "\n",
    "# Cache the dataset in memory (or use a directory to store it on disk if necessary)\n",
    "train_ds_hr = full_ds_hr.take(train_size_hr).shuffle(train_size_hr, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_hr = full_ds_hr.skip(train_size_hr).take(val_size_hr).shuffle(val_size_hr, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds_hr = full_ds_hr.skip(train_size_hr + val_size_hr).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples_hr(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples_hr(train_ds_hr)} samples in the High Training set High Resolution')\n",
    "print(f'Using {count_samples_hr(val_ds_hr)} samples in the High Validation set High Resolution')\n",
    "print(f'Using {count_samples_hr(test_ds_hr)} samples in the High Test set High Resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset: Deep High Resolution - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = 'savedDatasetDeepHR'\n",
    "\n",
    "tf.data.experimental.save(train_ds_hr, path_to_save + '/train')\n",
    "tf.data.experimental.save(val_ds_hr, path_to_save + '/val')\n",
    "tf.data.experimental.save(test_ds_hr, path_to_save + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 16)      448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 128, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64, 64, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4, 4, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2, 2, 512)         262656    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                36882     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,876,338\n",
      "Trainable params: 1,874,322\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN Model from Scratch\n",
    "def build_scratch_cnn_deep_hr():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE_HR[0], IMG_SIZE_HR[1], 3)))\n",
    "    # model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    \n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(256, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Conv2D(512, 3, padding='same', activation='relu'))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=elastic_net_regularizer))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with an initial learning rate\n",
    "    model.compile(\n",
    "        # Adam optimizer is used with a specified initial learning rate of 0.001. The learning rate\n",
    "        # controls how much the weights of the model are adjusted relative to the gradient of the loss \n",
    "        # function. A higher learning rate might converge quickly, but too high can cause the training \n",
    "        # to diverge. A lower learning rate ensures more reliable convergence but at the risk of slowing\n",
    "        # down the training process. The chosen rate of 0.001 is a starting point that balances these factors.\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_deep_hr = build_scratch_cnn_deep_hr()\n",
    "scratch_model_deep_hr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep High Resolution - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 124s 43ms/step - loss: 4.2533 - accuracy: 0.4261 - val_loss: 1.6990 - val_accuracy: 0.7214 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 59s 21ms/step - loss: 1.3674 - accuracy: 0.7896 - val_loss: 1.2037 - val_accuracy: 0.8406 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 55s 20ms/step - loss: 1.1050 - accuracy: 0.8490 - val_loss: 1.1063 - val_accuracy: 0.8610 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 54s 20ms/step - loss: 0.9612 - accuracy: 0.8784 - val_loss: 0.8669 - val_accuracy: 0.9065 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 56s 20ms/step - loss: 0.8710 - accuracy: 0.8961 - val_loss: 0.8882 - val_accuracy: 0.9058 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 55s 20ms/step - loss: 0.7914 - accuracy: 0.9101 - val_loss: 0.8826 - val_accuracy: 0.8862 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 56s 21ms/step - loss: 0.7325 - accuracy: 0.9198 - val_loss: 0.7137 - val_accuracy: 0.9282 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 55s 20ms/step - loss: 0.6757 - accuracy: 0.9273 - val_loss: 0.7149 - val_accuracy: 0.9218 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 54s 20ms/step - loss: 0.6093 - accuracy: 0.9376 - val_loss: 0.6388 - val_accuracy: 0.9358 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 53s 19ms/step - loss: 0.5661 - accuracy: 0.9421 - val_loss: 0.6214 - val_accuracy: 0.9307 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_deep_hr = scratch_model_deep_hr.fit(\n",
    "    train_ds_hr,\n",
    "    validation_data=val_ds_hr,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CNN Model: Deep High Resolution - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_deep_hr.history.keys():\n",
    "    history_deep_hr.history[key] = [float(i) for i in history_deep_hr.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/cnn_model_deep_hr.json', 'w') as f:\n",
    "    json.dump(history_deep_hr.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "scratch_model_deep_hr.save('h5/scratch_model_deep_hr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
