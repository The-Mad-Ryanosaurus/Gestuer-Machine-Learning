{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomFlip, Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set seed\n",
    "SEED = 338424\n",
    "\n",
    "# Global variables\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 18 # Number of folders in dataset\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the Training set\n",
      "Using 25184 samples in the Validation set\n",
      "Using 12600 samples in the Test set\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size = len(full_ds)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - (train_size + val_size)\n",
    "\n",
    "# # Split the dataset\n",
    "# train_ds = full_ds.take(train_size)\n",
    "# val_ds = full_ds.skip(train_size).take(val_size)\n",
    "# test_ds = full_ds.skip(train_size + val_size)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds = full_ds.take(train_size).shuffle(train_size, seed=SEED)\n",
    "val_ds = full_ds.skip(train_size).take(val_size).shuffle(val_size, seed=SEED)\n",
    "test_ds = full_ds.skip(train_size + val_size).shuffle(test_size, seed=SEED)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples(train_ds)} samples in the Training set')\n",
    "print(f'Using {count_samples(val_ds)} samples in the Validation set')\n",
    "print(f'Using {count_samples(test_ds)} samples in the Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'dislike',\n",
       " 'fist',\n",
       " 'four',\n",
       " 'like',\n",
       " 'mute',\n",
       " 'ok',\n",
       " 'one',\n",
       " 'palm',\n",
       " 'peace',\n",
       " 'peace_inverted',\n",
       " 'rock',\n",
       " 'stop',\n",
       " 'stop_inverted',\n",
       " 'three',\n",
       " 'three2',\n",
       " 'two_up',\n",
       " 'two_up_inverted']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = full_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Light CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                1170      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,410\n",
      "Trainable params: 287,186\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Light CNN Model from Scratch\n",
    "def build_scratch_cnn_light():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  # Reduced filter size\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  # Smaller second layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  # Smaller third layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))  # Reduced fully connected layer\n",
    "    model.add(layers.Dropout(0.3))  # Reduced dropout\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_light = build_scratch_cnn_light()\n",
    "scratch_model_light.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 73s 17ms/step - loss: 2.7318 - accuracy: 0.1088 - val_loss: 2.5562 - val_accuracy: 0.1630\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 69s 17ms/step - loss: 2.5284 - accuracy: 0.1635 - val_loss: 2.4230 - val_accuracy: 0.1966\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 69s 17ms/step - loss: 2.3337 - accuracy: 0.2251 - val_loss: 2.2527 - val_accuracy: 0.2769\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 69s 17ms/step - loss: 1.9908 - accuracy: 0.3259 - val_loss: 1.7009 - val_accuracy: 0.4507\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 70s 18ms/step - loss: 1.6145 - accuracy: 0.4530 - val_loss: 1.7430 - val_accuracy: 0.4587\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 70s 18ms/step - loss: 1.3367 - accuracy: 0.5484 - val_loss: 1.0862 - val_accuracy: 0.6582\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 71s 18ms/step - loss: 1.1605 - accuracy: 0.6104 - val_loss: 1.0368 - val_accuracy: 0.6710\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 71s 18ms/step - loss: 1.0376 - accuracy: 0.6515 - val_loss: 1.0190 - val_accuracy: 0.6789\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 70s 17ms/step - loss: 0.9439 - accuracy: 0.6821 - val_loss: 0.9437 - val_accuracy: 0.6979\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 70s 18ms/step - loss: 0.8721 - accuracy: 0.7061 - val_loss: 0.8642 - val_accuracy: 0.7307\n"
     ]
    }
   ],
   "source": [
    "# Train Light CNN Model\n",
    "history_custom = scratch_model_light.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 34s 5ms/step - loss: 0.8619 - accuracy: 0.7297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8618804812431335, 0.7296825647354126]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Light CNN Model\n",
    "scratch_model_light.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_light.save('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_light = load_model('scratch_model_light.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 315ms/step\n",
      "Prediction for myImages/three2.png: three2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction for myImages/rock.png: peace_inverted\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/ok.jpg: call\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_light, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 128, 128, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 64, 64, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 64, 64, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 64, 64, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 4, 4, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,786\n",
      "Trainable params: 355,890\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5)) \n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_deep = build_scratch_cnn()\n",
    "scratch_model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 146s 42ms/step - loss: 2.7876 - accuracy: 0.1516 - val_loss: 2.1395 - val_accuracy: 0.3644\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 115s 33ms/step - loss: 1.7598 - accuracy: 0.5011 - val_loss: 1.4110 - val_accuracy: 0.6528\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 119s 34ms/step - loss: 1.2307 - accuracy: 0.7230 - val_loss: 0.9232 - val_accuracy: 0.8284\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 122s 35ms/step - loss: 1.0267 - accuracy: 0.7932 - val_loss: 0.8557 - val_accuracy: 0.8444\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 126s 36ms/step - loss: 0.9197 - accuracy: 0.8278 - val_loss: 0.8916 - val_accuracy: 0.8324\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 121s 35ms/step - loss: 0.8551 - accuracy: 0.8466 - val_loss: 0.7245 - val_accuracy: 0.8816\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 123s 35ms/step - loss: 0.8029 - accuracy: 0.8606 - val_loss: 0.7056 - val_accuracy: 0.8861\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 131s 38ms/step - loss: 0.7602 - accuracy: 0.8690 - val_loss: 0.7082 - val_accuracy: 0.8854\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 118s 33ms/step - loss: 0.7282 - accuracy: 0.8765 - val_loss: 0.6870 - val_accuracy: 0.8862\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 119s 34ms/step - loss: 0.7000 - accuracy: 0.8845 - val_loss: 0.6821 - val_accuracy: 0.8848\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_deep.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 46s 6ms/step - loss: 0.6876 - accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6875975131988525, 0.8775396943092346]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Deep CNN Model\n",
    "scratch_model_deep.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_deep.save('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_deep = load_model('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Prediction for myImages/three2.png: three2\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Prediction for myImages/rock.png: rock\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/ok.jpg: ok\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_deep, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_6 (Sequential)   (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling_3 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 64, 64, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,634\n",
      "Trainable params: 306,738\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5)) \n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_da = build_scratch_cnn()\n",
    "scratch_model_da.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "2754/2754 [==============================] - 428s 133ms/step - loss: 2.9181 - accuracy: 0.1024 - val_loss: 2.4708 - val_accuracy: 0.2285\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 385s 131ms/step - loss: 2.2598 - accuracy: 0.3096 - val_loss: 1.6261 - val_accuracy: 0.5374\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 383s 131ms/step - loss: 1.7588 - accuracy: 0.5098 - val_loss: 1.3032 - val_accuracy: 0.6725\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 384s 131ms/step - loss: 1.5329 - accuracy: 0.5959 - val_loss: 1.1477 - val_accuracy: 0.7220\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 379s 130ms/step - loss: 1.4032 - accuracy: 0.6475 - val_loss: 1.1871 - val_accuracy: 0.7120\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 379s 130ms/step - loss: 1.3087 - accuracy: 0.6809 - val_loss: 1.0383 - val_accuracy: 0.7568\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 381s 131ms/step - loss: 1.2507 - accuracy: 0.7000 - val_loss: 1.0382 - val_accuracy: 0.7641\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 381s 131ms/step - loss: 1.2022 - accuracy: 0.7147 - val_loss: 0.9256 - val_accuracy: 0.7980\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 381s 131ms/step - loss: 1.1581 - accuracy: 0.7272 - val_loss: 0.8811 - val_accuracy: 0.8106\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 437s 151ms/step - loss: 1.1333 - accuracy: 0.7354 - val_loss: 0.9517 - val_accuracy: 0.7934\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_da.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 41s 6ms/step - loss: 0.9589 - accuracy: 0.7879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.958897054195404, 0.7878571152687073]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the CNN Data Augmentation Deep Model\n",
    "scratch_model_da.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_da.save('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_da = load_model('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_da, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation without Dropout/Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_13 (Sequential)  (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling_7 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 64, 64, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_38 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,122\n",
      "Trainable params: 290,226\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "   \n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    \n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "  \n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_da2 = build_scratch_cnn()\n",
    "scratch_model_da2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "2754/2754 [==============================] - 379s 129ms/step - loss: 1.9842 - accuracy: 0.3670 - val_loss: 1.2109 - val_accuracy: 0.6120\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 374s 128ms/step - loss: 1.0563 - accuracy: 0.6568 - val_loss: 0.8299 - val_accuracy: 0.7273\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 377s 129ms/step - loss: 0.8142 - accuracy: 0.7367 - val_loss: 0.7506 - val_accuracy: 0.7550\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 376s 129ms/step - loss: 0.7357 - accuracy: 0.7595 - val_loss: 0.9575 - val_accuracy: 0.6875\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 375s 128ms/step - loss: 0.6553 - accuracy: 0.7878 - val_loss: 0.6981 - val_accuracy: 0.7694\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 378s 129ms/step - loss: 0.6018 - accuracy: 0.8053 - val_loss: 0.5978 - val_accuracy: 0.8110\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 375s 128ms/step - loss: 0.5512 - accuracy: 0.8204 - val_loss: 0.5299 - val_accuracy: 0.8310\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 378s 130ms/step - loss: 0.5161 - accuracy: 0.8320 - val_loss: 0.6331 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 376s 129ms/step - loss: 0.4919 - accuracy: 0.8409 - val_loss: 0.5971 - val_accuracy: 0.8061\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 440s 152ms/step - loss: 0.4678 - accuracy: 0.8482 - val_loss: 0.5190 - val_accuracy: 0.8325\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_da2.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 36s 10ms/step - loss: 0.5184 - accuracy: 0.8387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5184229016304016, 0.8386508226394653]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the CNN Data Augmentation Deep Model\n",
    "scratch_model_da2.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_da2.save('scratch_model_da2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_da2 = load_model('scratch_model_da2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Prediction for myImages/three2.png: peace\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/rock.png: rock\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Prediction for myImages/ok.jpg: ok\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_da2, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
