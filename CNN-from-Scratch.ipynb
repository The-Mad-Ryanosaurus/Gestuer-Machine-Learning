{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning: Convolutional Neural Network Gesture Models**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ryan Harte**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [CNN: Project Setup](#cnn-project-setup)\n",
    "3. [CNN Model: Light from Scratch](#cnn-model-light-from-scratch)\n",
    "4. [CNN Model: Deep from Scratch](#cnn-model-deep-from-scratch)\n",
    "5. [CNN Model: Data Augmentation](#cnn-model-data-augmentation)\n",
    "6. [CNN Model: Data Augmentation Fine Tuned](#cnn-model-data-augmentation-fine-tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "This repository features a Jupyter Notebook that explores Convolutional Neural Networks (CNNs) in machine learning. The notebook analyzes a comprehensive dataset of images depicting human hand gestures and training a Neural Network (NN) to accurately recognize them. It is then tested on images of my own hand gestures to assess the NN ability to identify them correctly. The project also includes an evaluation of the model's accuracy and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN: Project Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomFlip, Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from the logging module\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if any GPU devices are detected\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set seed\n",
    "SEED = 338424\n",
    "\n",
    "# Global variables\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 18 # Number of folders in dataset\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset: Loading, Splitting and Shuffling the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the Training set\n",
      "Using 25184 samples in the Validation set\n",
      "Using 12600 samples in the Test set\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size = len(full_ds)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - (train_size + val_size)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds = full_ds.take(train_size).shuffle(train_size, seed=SEED)\n",
    "val_ds = full_ds.skip(train_size).take(val_size).shuffle(val_size, seed=SEED)\n",
    "test_ds = full_ds.skip(train_size + val_size).shuffle(test_size, seed=SEED)\n",
    "\n",
    "# Cache the dataset in memory (or use a directory to store it on disk if necessary)\n",
    "train_ds = full_ds.take(train_size).shuffle(train_size, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = full_ds.skip(train_size).take(val_size).shuffle(val_size, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = full_ds.skip(train_size + val_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples(train_ds)} samples in the Training set')\n",
    "print(f'Using {count_samples(val_ds)} samples in the Validation set')\n",
    "print(f'Using {count_samples(test_ds)} samples in the Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'dislike',\n",
       " 'fist',\n",
       " 'four',\n",
       " 'like',\n",
       " 'mute',\n",
       " 'ok',\n",
       " 'one',\n",
       " 'palm',\n",
       " 'peace',\n",
       " 'peace_inverted',\n",
       " 'rock',\n",
       " 'stop',\n",
       " 'stop_inverted',\n",
       " 'three',\n",
       " 'three2',\n",
       " 'two_up',\n",
       " 'two_up_inverted']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = full_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Light from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                1170      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,410\n",
      "Trainable params: 287,186\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Light CNN Model from Scratch\n",
    "def build_scratch_cnn_light():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))  # Reduced filter size\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))  # Smaller second layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))  # Smaller third layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))  # Reduced fully connected layer\n",
    "    model.add(layers.Dropout(0.3))  # Reduced dropout\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the lighter model\n",
    "scratch_model_light = build_scratch_cnn_light()\n",
    "scratch_model_light.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 21s 7ms/step - loss: 2.8933 - accuracy: 0.0570 - val_loss: 2.8903 - val_accuracy: 0.0590\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 17s 6ms/step - loss: 2.8910 - accuracy: 0.0572 - val_loss: 2.8903 - val_accuracy: 0.0590\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 17s 6ms/step - loss: 2.8904 - accuracy: 0.0572 - val_loss: 2.8903 - val_accuracy: 0.0590\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 17s 6ms/step - loss: 2.8904 - accuracy: 0.0572 - val_loss: 2.8903 - val_accuracy: 0.0590\n"
     ]
    }
   ],
   "source": [
    "# Train Light CNN Model\n",
    "history_custom = scratch_model_light.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Light from Scratch Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 1s 3ms/step - loss: 2.8904 - accuracy: 0.0562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8903844356536865, 0.056190475821495056]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Light CNN Model\n",
    "scratch_model_light.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Light Model: Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_light.save('scratch_model_light.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_light = load_model('scratch_model_light.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Light from Scratch Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 149ms/step\n",
      "Prediction for myImages/three2.png: two_up\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/rock.png: two_up\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction for myImages/ok.jpg: two_up\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_light, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Deep from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64, 64, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,634\n",
      "Trainable params: 306,738\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_cnn_deep():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5)) \n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_deep = build_scratch_cnn_deep()\n",
    "scratch_model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 28s 10ms/step - loss: 2.8192 - accuracy: 0.1452 - val_loss: 2.4518 - val_accuracy: 0.2472\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 27s 10ms/step - loss: 1.8776 - accuracy: 0.4605 - val_loss: 1.5553 - val_accuracy: 0.5991\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 27s 10ms/step - loss: 1.3493 - accuracy: 0.6655 - val_loss: 1.0791 - val_accuracy: 0.7606\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 1.1407 - accuracy: 0.7368 - val_loss: 1.0912 - val_accuracy: 0.7525\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 1.0291 - accuracy: 0.7752 - val_loss: 0.9563 - val_accuracy: 0.7929\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 0.9571 - accuracy: 0.7972 - val_loss: 0.9157 - val_accuracy: 0.8065\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 0.9062 - accuracy: 0.8116 - val_loss: 0.8730 - val_accuracy: 0.8175\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 0.8701 - accuracy: 0.8229 - val_loss: 0.8877 - val_accuracy: 0.8162\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 26s 10ms/step - loss: 0.8314 - accuracy: 0.8337 - val_loss: 0.7858 - val_accuracy: 0.8402\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 26s 9ms/step - loss: 0.8061 - accuracy: 0.8415 - val_loss: 0.8397 - val_accuracy: 0.8292\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_deep.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep from Scratch Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/394 [..............................] - ETA: 7s - loss: 0.9141 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 2s 4ms/step - loss: 0.8659 - accuracy: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8658940196037292, 0.8226190209388733]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Deep CNN Model\n",
    "scratch_model_deep.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Deep Model: Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_deep.save('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_deep = load_model('scratch_model_deep.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Deep from Scratch Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n",
      "Prediction for myImages/three2.png: three2\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Prediction for myImages/rock.png: rock\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Prediction for myImages/ok.jpg: call\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_deep, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_2 (Sequential)   (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling_2 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64, 64, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,634\n",
      "Trainable params: 306,738\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_cnn_da():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5)) \n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_da = build_scratch_cnn_da()\n",
    "scratch_model_da.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 305s 110ms/step - loss: 2.8940 - accuracy: 0.1170 - val_loss: 2.5491 - val_accuracy: 0.2025\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 307s 111ms/step - loss: 2.0872 - accuracy: 0.3771 - val_loss: 1.5178 - val_accuracy: 0.6123\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 306s 111ms/step - loss: 1.5462 - accuracy: 0.5936 - val_loss: 1.4807 - val_accuracy: 0.6397\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 302s 110ms/step - loss: 1.3161 - accuracy: 0.6779 - val_loss: 1.0757 - val_accuracy: 0.7542\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 299s 109ms/step - loss: 1.1897 - accuracy: 0.7224 - val_loss: 1.0930 - val_accuracy: 0.7459\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 301s 109ms/step - loss: 1.1184 - accuracy: 0.7465 - val_loss: 1.0134 - val_accuracy: 0.7724\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 301s 109ms/step - loss: 1.0543 - accuracy: 0.7632 - val_loss: 0.8609 - val_accuracy: 0.8180\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 299s 109ms/step - loss: 1.0094 - accuracy: 0.7785 - val_loss: 0.8721 - val_accuracy: 0.8162\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 300s 109ms/step - loss: 0.9831 - accuracy: 0.7865 - val_loss: 0.7841 - val_accuracy: 0.8446\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 301s 109ms/step - loss: 0.9532 - accuracy: 0.7949 - val_loss: 0.8245 - val_accuracy: 0.8315\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_da.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Data Augmentation Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 2s 4ms/step - loss: 0.8310 - accuracy: 0.8278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8310316801071167, 0.8277778029441833]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the CNN Data Augmentation Deep Model\n",
    "scratch_model_da.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Data Augmentation Model: Save and Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_da.save('scratch_model_da.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_da = load_model('scratch_model_da.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Data Augmentation Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n",
      "Prediction for myImages/three2.png: three2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/rock.png: rock\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction for myImages/ok.jpg: ok\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_da, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Data Augmentation Fine Tuned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_2 (Sequential)   (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling_3 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 64, 64, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,122\n",
      "Trainable params: 290,226\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Deep CNN Model from Scratch\n",
    "def build_scratch_model_da_adapted_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "    model.add(data_augmentation_layers)\n",
    "    model.add(layers.Rescaling(1.0 / 255))  # Normalize pixel values\n",
    "   \n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "   \n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    \n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "  \n",
    "    model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "scratch_model_da_adapted = build_scratch_model_da_adapted_cnn()\n",
    "scratch_model_da_adapted.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 286s 103ms/step - loss: 1.7051 - accuracy: 0.4550 - val_loss: 1.1785 - val_accuracy: 0.6275\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 287s 104ms/step - loss: 0.8714 - accuracy: 0.7182 - val_loss: 1.0365 - val_accuracy: 0.6769\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 289s 105ms/step - loss: 0.6705 - accuracy: 0.7835 - val_loss: 0.5975 - val_accuracy: 0.8054\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 291s 106ms/step - loss: 0.5771 - accuracy: 0.8129 - val_loss: 0.5666 - val_accuracy: 0.8203\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 288s 105ms/step - loss: 0.5087 - accuracy: 0.8348 - val_loss: 0.5800 - val_accuracy: 0.8159\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 288s 104ms/step - loss: 0.4684 - accuracy: 0.8471 - val_loss: 0.5869 - val_accuracy: 0.8104\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 287s 104ms/step - loss: 0.4228 - accuracy: 0.8610 - val_loss: 0.4188 - val_accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 295s 107ms/step - loss: 0.3853 - accuracy: 0.8739 - val_loss: 0.4606 - val_accuracy: 0.8518\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 288s 104ms/step - loss: 0.3605 - accuracy: 0.8821 - val_loss: 0.4692 - val_accuracy: 0.8513\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 289s 105ms/step - loss: 0.3362 - accuracy: 0.8892 - val_loss: 0.4026 - val_accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# Train Deep CNN Model\n",
    "history_custom = scratch_model_da_adapted.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Data Augmentation Fine Tuned Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 2s 4ms/step - loss: 0.4104 - accuracy: 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4103633761405945, 0.8656349182128906]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the CNN Data Augmentation Deep Model\n",
    "scratch_model_da_adapted.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Data Augmentation Model Adapted: Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Deep CNN Model\n",
    "scratch_model_da_adapted.save('scratch_model_da_adapted.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "scratch_model_da_adapted = load_model('scratch_model_da_adapted.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Data Augmentation Fine Tuned Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "Prediction for myImages/three2.png: three2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction for myImages/rock.png: rock\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Prediction for myImages/ok.jpg: ok\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Prediction for myImages/two_up_inverted.png: two_up_inverted\n"
     ]
    }
   ],
   "source": [
    "# Testing own images\n",
    "def predict_gesture(model, img_path, class_names):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "gesture_images = ['myImages/three2.png',\n",
    "                  'myImages/rock.png',\n",
    "                  'myImages/ok.jpg',\n",
    "                  'myImages/two_up_inverted.png']\n",
    "\n",
    "for img_path in gesture_images:\n",
    "    predicted_gesture = predict_gesture(scratch_model_da_adapted, img_path, class_names)\n",
    "    print(f\"Prediction for {img_path}: {predicted_gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing Model in Realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming scratch_model_da2 and class_names are already defined elsewhere\n",
    "# cap is the VideoCapture object\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Adjust camera settings if needed\n",
    "cap.set(3, 480)  # Adjust width\n",
    "cap.set(4, 480)  # Adjust height\n",
    "\n",
    "while True:\n",
    "    # Read the frame from the camera\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Ensure that the frame was captured successfully before proceeding\n",
    "    if not success:\n",
    "        print(\"Error: Unable to capture video\")\n",
    "        break\n",
    "\n",
    "    # Resize the image to the desired size\n",
    "    imgsmall = cv2.resize(img, (128, 128))\n",
    "\n",
    "    # Prepare the image for prediction\n",
    "    image = np.array(imgsmall)\n",
    "    image = image.reshape(1, 128, 128, 3)\n",
    "\n",
    "    # Predict with your model\n",
    "    ans = scratch_model_deep.predict(image)\n",
    "    val = pd.DataFrame(ans, columns=class_names).idxmax(axis=1)\n",
    "\n",
    "    # Display the prediction on the image\n",
    "    cv2.putText(img, val.values[0], (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Show the webcam feed\n",
    "    cv2.imshow(\"Webcam\", img)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN Greyscale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transfer Learning VGG-16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
