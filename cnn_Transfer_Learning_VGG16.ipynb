{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries and Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, applications\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomFlip, Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "\n",
    "# Suppress warnings from the logging module\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tensorflow Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPU Checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if any GPU devices are detected\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging except for fatal errors.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 338424\n",
    "\n",
    "# Global variables\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 18 # Number of folders in dataset\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VGG-16 Dataset: Loading, Splitting, Shuffling, Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125912 files belonging to 18 classes.\n",
      "Using 88128 samples in the Training set\n",
      "Using 25184 samples in the Validation set\n",
      "Using 12600 samples in the Test set\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_dir = 'dataset/hagridset'\n",
    "full_ds_vgg16 = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_ratio_vgg16 = 0.7\n",
    "val_ratio_vgg16 = 0.2\n",
    "test_ratio_vgg16 = 0.1\n",
    "\n",
    "# Total length of the dataset\n",
    "total_size_vgg16 = len(full_ds_vgg16)\n",
    "\n",
    "# Compute indices for the splits\n",
    "train_size_vgg16 = int(total_size_vgg16 * train_ratio_vgg16)\n",
    "val_size_vgg16 = int(total_size_vgg16 * val_ratio_vgg16)\n",
    "test_size_vgg16 = total_size_vgg16 - (train_size_vgg16 + val_size_vgg16)\n",
    "\n",
    "# Split the dataset and shuffle\n",
    "train_ds_vgg16 = full_ds_vgg16.take(train_size_vgg16).shuffle(train_size_vgg16, seed=SEED)\n",
    "val_ds_vgg16 = full_ds_vgg16.skip(train_size_vgg16).take(val_size_vgg16).shuffle(val_size_vgg16, seed=SEED)\n",
    "test_ds_vgg16 = full_ds_vgg16.skip(train_size_vgg16 + val_size_vgg16).shuffle(test_size_vgg16, seed=SEED)\n",
    "\n",
    "# Cache the dataset in memory (or use a directory to store it on disk if necessary)\n",
    "train_ds_vgg16 = full_ds_vgg16.take(train_size_vgg16).shuffle(train_size_vgg16, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_vgg16 = full_ds_vgg16.skip(train_size_vgg16).take(val_size_vgg16).shuffle(val_size_vgg16, seed=SEED).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds_vgg16 = full_ds_vgg16.skip(train_size_vgg16 + val_size_vgg16).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Count samples in each subset\n",
    "def count_samples(dataset):\n",
    "    sample_count = sum(1 for _ in dataset.unbatch())\n",
    "    return sample_count\n",
    "\n",
    "# Output the number of samples for each dataset\n",
    "print(f'Using {count_samples(train_ds_vgg16)} samples in the Training set')\n",
    "print(f'Using {count_samples(val_ds_vgg16)} samples in the Validation set')\n",
    "print(f'Using {count_samples(test_ds_vgg16)} samples in the Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'dislike',\n",
       " 'fist',\n",
       " 'four',\n",
       " 'like',\n",
       " 'mute',\n",
       " 'ok',\n",
       " 'one',\n",
       " 'palm',\n",
       " 'peace',\n",
       " 'peace_inverted',\n",
       " 'rock',\n",
       " 'stop',\n",
       " 'stop_inverted',\n",
       " 'three',\n",
       " 'three2',\n",
       " 'two_up',\n",
       " 'two_up_inverted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = full_ds_vgg16.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset: Transfer Learning VGG-16 - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = 'savedDatasetTLVGG16'\n",
    "\n",
    "tf.data.experimental.save(train_ds_vgg16, path_to_save + '/train')\n",
    "tf.data.experimental.save(val_ds_vgg16, path_to_save + '/val')\n",
    "tf.data.experimental.save(test_ds_vgg16, path_to_save + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset Grayscale Models were trained on\n",
    "path_to_load_TLVGG16_dataset = 'savedDatasetTLVGG16'\n",
    "train_ds_vgg16 = tf.data.experimental.load(path_to_load_TLVGG16_dataset + '/train')\n",
    "val_ds_vgg16 = tf.data.experimental.load(path_to_load_TLVGG16_dataset + '/val')\n",
    "test_ds_vgg16 = tf.data.experimental.load(path_to_load_TLVGG16_dataset + '/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regularization Factors**\n",
    "\n",
    "This code snippet defines the values for L1 and L2 regularization, which are both set to 0.01. It then creates an \"Elastic Net Regularizer\" that combines these L1 and L2 values to help prevent the model from overfitting by penalizing overly complex or large weight values in the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L1 and L2 regularization factors\n",
    "l1_factor = 0.01  # Example value\n",
    "l2_factor = 0.01  # Example value\n",
    "\n",
    "# Elastic Net Regularizer\n",
    "elastic_net_regularizer = regularizers.l1_l2(l1=l1_factor, l2=l2_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Callbacks: Learning Rate Scheduler and Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def lr_time_based_decay(epoch, lr):\n",
    "    # This function adjusts the learning rate over each epoch based on the initial learning rate,\n",
    "    # applying a decay factor that increases with the epoch number. It effectively reduces the \n",
    "    # learning rate over time, which can help in calibrating the model adjustments as it \n",
    "    # approaches a minimum in the loss surface.\n",
    "    return lr * 1 / (1 + 0.01 * epoch)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # EarlyStopping prevents overfitting by stopping training when the validation loss \n",
    "    # has not improved for 3 consecutive epochs ('patience=3'). It also restores the \n",
    "    # weights of the model to those of the epoch with the best validation loss, ensuring \n",
    "    # the model retains the best learned features even if it starts to overfit afterward.\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    # LearningRateScheduler adjusts the learning rate according to the lr_time_based_decay function above.\n",
    "    # It logs the new learning rate at the start of each epoch ('verbose=1'), helping to control\n",
    "    # the step size of model updates, which can be crucial for reaching convergence efficiently.\n",
    "    LearningRateScheduler(lr_time_based_decay, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Augmentation Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation_layers = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Convolutional Neural Networks (CNN): Transfer Learning VGG-16**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Transfer Learning VGG-16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 128, 128, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 128, 128, 3)      0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,723,922\n",
      "Trainable params: 9,234\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the base model\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False\n",
    ")\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Define the inputs\n",
    "inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Preprocessing input according to VGG16\n",
    "x = preprocess_input(inputs)\n",
    "\n",
    "# Pass the inputs through the base model\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Add GlobalAveragePooling2D layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_transfer_learning_vgg16 = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_transfer_learning_vgg16.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "model_transfer_learning_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Transfer Learning VGG-16 - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 92s 33ms/step - loss: 2.3931 - accuracy: 0.4368 - val_loss: 1.6101 - val_accuracy: 0.5322 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009900990569281696.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 81s 30ms/step - loss: 1.6405 - accuracy: 0.5213 - val_loss: 1.5788 - val_accuracy: 0.5395 - lr: 9.9010e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009706853341092082.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 83s 30ms/step - loss: 1.6218 - accuracy: 0.5249 - val_loss: 1.5807 - val_accuracy: 0.5400 - lr: 9.7069e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009424129424128428.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 86s 31ms/step - loss: 1.6114 - accuracy: 0.5267 - val_loss: 1.5777 - val_accuracy: 0.5406 - lr: 9.4241e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009061662869778676.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 85s 31ms/step - loss: 1.5983 - accuracy: 0.5289 - val_loss: 1.5705 - val_accuracy: 0.5421 - lr: 9.0617e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008630154964824517.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 84s 31ms/step - loss: 1.5825 - accuracy: 0.5311 - val_loss: 1.5603 - val_accuracy: 0.5439 - lr: 8.6302e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0008141655444149982.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 84s 31ms/step - loss: 1.5645 - accuracy: 0.5337 - val_loss: 1.5483 - val_accuracy: 0.5463 - lr: 8.1417e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000760902402591761.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 84s 31ms/step - loss: 1.5448 - accuracy: 0.5369 - val_loss: 1.5351 - val_accuracy: 0.5486 - lr: 7.6090e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0007045392757626595.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 85s 31ms/step - loss: 1.5239 - accuracy: 0.5400 - val_loss: 1.5211 - val_accuracy: 0.5501 - lr: 7.0454e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006463663297953135.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 81s 29ms/step - loss: 1.5024 - accuracy: 0.5441 - val_loss: 1.5071 - val_accuracy: 0.5519 - lr: 6.4637e-04\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_vgg16 = model_transfer_learning_vgg16.fit(\n",
    "    train_ds_vgg16,\n",
    "    validation_data=val_ds_vgg16,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Transfer Learning VGG-16 - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_vgg16.history.keys():\n",
    "    history_vgg16.history[key] = [float(i) for i in history_vgg16.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/tl_vgg16_model_shallow.json', 'w') as f:\n",
    "    json.dump(history_vgg16.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "model_transfer_learning_vgg16.save('h5/tl_vgg16_model_shallow.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN Model: Transfer Learning VGG-16 Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_transfer_learning_vgg16_fine_tuned\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              8389632   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 18)                9234      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,638,354\n",
      "Trainable params: 11,283,474\n",
      "Non-trainable params: 12,354,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_transfer_model(input_shape, num_classes):\n",
    "    # Load the VGG16 base model with ImageNet weights\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet', \n",
    "        input_shape=input_shape, \n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    # Unfreeze the last 8 layers for fine-tuning\n",
    "    base_model.trainable = True  # Make the model trainable\n",
    "    for layer in base_model.layers[:-2]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[-2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Define the input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Pass the input through the base model\n",
    "    x = base_model(inputs, training=False)  # Use training=False to ensure that batchnorm layers do not update during forward pass\n",
    "\n",
    "    # Flatten the output of the base model\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Add the first dense layer\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=elastic_net_regularizer)(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "\n",
    "    # Add the second dense layer\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=elastic_net_regularizer)(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "\n",
    "    # Add the output layer\n",
    "    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)  # Use float32 for dtype to ensure precision\n",
    "\n",
    "    # Construct the model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"model_transfer_learning_vgg16_fine_tuned\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Constants\n",
    "input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "num_classes = 18  # Number of classes in the dataset\n",
    "\n",
    "# Create the model\n",
    "model_transfer_learning_vgg16_fine_tuned = build_transfer_model(input_shape, num_classes)\n",
    "\n",
    "# Output the model summary\n",
    "model_transfer_learning_vgg16_fine_tuned.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Transfer Learning VGG-16 Fine Tuning - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 1/10\n",
      "2754/2754 [==============================] - 108s 39ms/step - loss: 137.1809 - accuracy: 0.6177 - val_loss: 2.5834 - val_accuracy: 0.7438 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.900989848889853e-05.\n",
      "Epoch 2/10\n",
      "2754/2754 [==============================] - 90s 33ms/step - loss: 2.2385 - accuracy: 0.7438 - val_loss: 2.0146 - val_accuracy: 0.7886 - lr: 9.9010e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.706852913094575e-05.\n",
      "Epoch 3/10\n",
      "2754/2754 [==============================] - 90s 33ms/step - loss: 1.9399 - accuracy: 0.7892 - val_loss: 1.8571 - val_accuracy: 0.8089 - lr: 9.7069e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.424129000286236e-05.\n",
      "Epoch 4/10\n",
      "2754/2754 [==============================] - 89s 32ms/step - loss: 1.7782 - accuracy: 0.8175 - val_loss: 1.7663 - val_accuracy: 0.8193 - lr: 9.4241e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.06166231008963e-05.\n",
      "Epoch 5/10\n",
      "2754/2754 [==============================] - 91s 33ms/step - loss: 1.6556 - accuracy: 0.8359 - val_loss: 1.6844 - val_accuracy: 0.8244 - lr: 9.0617e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 8.63015468764518e-05.\n",
      "Epoch 6/10\n",
      "2754/2754 [==============================] - 92s 33ms/step - loss: 1.5484 - accuracy: 0.8528 - val_loss: 1.6274 - val_accuracy: 0.8275 - lr: 8.6302e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 8.141655169585543e-05.\n",
      "Epoch 7/10\n",
      "2754/2754 [==============================] - 90s 33ms/step - loss: 1.4557 - accuracy: 0.8656 - val_loss: 1.5616 - val_accuracy: 0.8323 - lr: 8.1417e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 7.609023753919194e-05.\n",
      "Epoch 8/10\n",
      "2754/2754 [==============================] - 88s 32ms/step - loss: 1.3694 - accuracy: 0.8778 - val_loss: 1.5156 - val_accuracy: 0.8313 - lr: 7.6090e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 7.045392622886639e-05.\n",
      "Epoch 9/10\n",
      "2754/2754 [==============================] - 87s 31ms/step - loss: 1.2849 - accuracy: 0.8890 - val_loss: 1.4702 - val_accuracy: 0.8321 - lr: 7.0454e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 6.463663030945517e-05.\n",
      "Epoch 10/10\n",
      "2754/2754 [==============================] - 86s 31ms/step - loss: 1.2083 - accuracy: 0.8975 - val_loss: 1.4207 - val_accuracy: 0.8329 - lr: 6.4637e-05\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Model\n",
    "history_vgg16_fine_tuned = model_transfer_learning_vgg16_fine_tuned.fit(\n",
    "    train_ds_vgg16,\n",
    "    validation_data=val_ds_vgg16,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Model: Transfer Learning VGG-16 Fine Tuning - Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model: .json\n",
    "# Saves the Model Architecture\n",
    "for key in history_vgg16_fine_tuned.history.keys():\n",
    "    history_vgg16_fine_tuned.history[key] = [float(i) for i in history_vgg16_fine_tuned.history[key]]\n",
    "\n",
    "# Write the JSON file\n",
    "with open('json/tl_vgg16_model_vgg16_fine_tuned.json', 'w') as f:\n",
    "    json.dump(history_vgg16_fine_tuned.history, f)\n",
    "\n",
    "\n",
    "# Save Model: .h5\n",
    "# Saves the Model Weights and Configurations\n",
    "model_transfer_learning_vgg16_fine_tuned.save('h5/tl_vgg16_model_fine_tuned.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
